"""
Enterprise Database System
BigTech-level database architecture for millions of users
"""
import asyncio
import logging
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any, Tuple, Union
from dataclasses import dataclass
from enum import Enum
import uuid
import json
import hashlib
import secrets
import base64

import redis.asyncio as redis
from sqlalchemy.ext.asyncio import AsyncSession, create_async_engine, async_sessionmaker
from sqlalchemy import select, update, delete, and_, or_, func, text
from sqlalchemy.dialects.postgresql import UUID, JSONB, ARRAY
from sqlalchemy.orm import DeclarativeBase, Mapped, mapped_column
from sqlalchemy.pool import QueuePool
from cryptography.fernet import Fernet
from cryptography.hazmat.primitives import hashes, serialization
from cryptography.hazmat.primitives.asymmetric import rsa, padding
from cryptography.hazmat.primitives.kdf.pbkdf2 import PBKDF2HMAC
from cryptography.hazmat.backends import default_backend
import argon2

from backend.config import settings

logger = logging.getLogger(__name__)


class DatabaseShard(Enum):
    """Database shards for horizontal scaling"""
    USERS = "users"
    MEETINGS = "meetings"
    ANALYTICS = "analytics"
    SOCIAL = "social"
    VOICES = "voices"
    BILLING = "billing"


class EncryptionLevel(Enum):
    """Encryption levels for different data types"""
    PUBLIC = "public"           # No encryption
    INTERNAL = "internal"       # AES-256
    SENSITIVE = "sensitive"     # AES-256 + RSA-4096
    CRITICAL = "critical"       # AES-256 + RSA-4096 + Argon2


@dataclass
class DatabaseConnection:
    """Database connection configuration"""
    host: str
    port: int
    database: str
    username: str
    password: str
    ssl_mode: str = "require"
    pool_size: int = 20
    max_overflow: int = 30
    pool_timeout: int = 30
    pool_recycle: int = 3600


@dataclass
class EncryptionKey:
    """Encryption key configuration"""
    key_id: str
    algorithm: str
    key_data: bytes
    created_at: datetime
    expires_at: Optional[datetime]
    is_active: bool = True


class EnterpriseDatabase:
    """Enterprise-grade database system with BigTech-level performance"""
    
    def __init__(self):
        self.engines = {}
        self.sessions = {}
        self.redis_clusters = {}
        self.encryption_keys = {}
        self.shard_routing = {}
        self.connection_pools = {}
        
        # Performance monitoring
        self.query_metrics = {}
        self.connection_metrics = {}
        self.encryption_metrics = {}
        
        # Security configuration
        self.encryption_config = {
            "aes_key_size": 32,  # 256 bits
            "rsa_key_size": 4096,  # 4096 bits
            "argon2_time_cost": 3,
            "argon2_memory_cost": 65536,  # 64 MB
            "argon2_parallelism": 4,
            "key_rotation_days": 90
        }
    
    async def initialize(self):
        """Initialize enterprise database system"""
        try:
            logger.info("ðŸš€ Initializing Enterprise Database System...")
            
            # Initialize database shards
            await self._initialize_database_shards()
            
            # Initialize Redis clusters
            await self._initialize_redis_clusters()
            
            # Initialize encryption system
            await self._initialize_encryption_system()
            
            # Initialize connection pools
            await self._initialize_connection_pools()
            
            # Start background tasks
            asyncio.create_task(self._health_monitor())
            asyncio.create_task(self._performance_monitor())
            asyncio.create_task(self._encryption_key_rotator())
            asyncio.create_task(self._backup_scheduler())
            
            logger.info("âœ… Enterprise Database System initialized successfully")
            
        except Exception as e:
            logger.error(f"âŒ Failed to initialize Enterprise Database System: {e}")
            raise
    
    async def _initialize_database_shards(self):
        """Initialize database shards for horizontal scaling"""
        try:
            # Primary database (users, authentication)
            primary_config = DatabaseConnection(
                host=settings.database_host,
                port=settings.database_port,
                database=f"{settings.database_name}_primary",
                username=settings.database_user,
                password=settings.database_password,
                pool_size=50,
                max_overflow=100
            )
            
            # Analytics database (high write volume)
            analytics_config = DatabaseConnection(
                host=settings.database_host,
                port=settings.database_port,
                database=f"{settings.database_name}_analytics",
                username=settings.database_user,
                password=settings.database_password,
                pool_size=100,
                max_overflow=200
            )
            
            # Social database (read-heavy)
            social_config = DatabaseConnection(
                host=settings.database_host,
                port=settings.database_port,
                database=f"{settings.database_name}_social",
                username=settings.database_user,
                password=settings.database_password,
                pool_size=30,
                max_overflow=50
            )
            
            # Meetings database (real-time)
            meetings_config = DatabaseConnection(
                host=settings.database_host,
                port=settings.database_port,
                database=f"{settings.database_name}_meetings",
                username=settings.database_user,
                password=settings.database_password,
                pool_size=40,
                max_overflow=80
            )
            
            # Billing database (critical data)
            billing_config = DatabaseConnection(
                host=settings.database_host,
                port=settings.database_port,
                database=f"{settings.database_name}_billing",
                username=settings.database_user,
                password=settings.database_password,
                pool_size=20,
                max_overflow=30
            )
            
            # Create engines for each shard
            shard_configs = {
                DatabaseShard.USERS: primary_config,
                DatabaseShard.ANALYTICS: analytics_config,
                DatabaseShard.SOCIAL: social_config,
                DatabaseShard.MEETINGS: meetings_config,
                DatabaseShard.BILLING: billing_config
            }
            
            for shard, config in shard_configs.items():
                engine = create_async_engine(
                    f"postgresql+asyncpg://{config.username}:{config.password}@{config.host}:{config.port}/{config.database}",
                    poolclass=QueuePool,
                    pool_size=config.pool_size,
                    max_overflow=config.max_overflow,
                    pool_timeout=config.pool_timeout,
                    pool_recycle=config.pool_recycle,
                    echo=False,
                    future=True
                )
                
                self.engines[shard] = engine
                self.sessions[shard] = async_sessionmaker(engine, expire_on_commit=False)
                
                logger.info(f"âœ… Database shard initialized: {shard.value}")
            
        except Exception as e:
            logger.error(f"Failed to initialize database shards: {e}")
            raise
    
    async def _initialize_redis_clusters(self):
        """Initialize Redis clusters for caching and session management"""
        try:
            # Primary Redis cluster (sessions, cache)
            primary_redis = redis.from_url(
                settings.redis_url,
                encoding="utf-8",
                decode_responses=True,
                max_connections=100,
                retry_on_timeout=True
            )
            
            # Analytics Redis cluster (real-time metrics)
            analytics_redis = redis.from_url(
                f"{settings.redis_url}/1",
                encoding="utf-8",
                decode_responses=True,
                max_connections=50
            )
            
            # Social Redis cluster (social features)
            social_redis = redis.from_url(
                f"{settings.redis_url}/2",
                encoding="utf-8",
                decode_responses=True,
                max_connections=30
            )
            
            # Encryption Redis cluster (encryption keys)
            encryption_redis = redis.from_url(
                f"{settings.redis_url}/3",
                encoding="utf-8",
                decode_responses=True,
                max_connections=20
            )
            
            self.redis_clusters = {
                "primary": primary_redis,
                "analytics": analytics_redis,
                "social": social_redis,
                "encryption": encryption_redis
            }
            
            # Test connections
            for name, cluster in self.redis_clusters.items():
                await cluster.ping()
                logger.info(f"âœ… Redis cluster initialized: {name}")
            
        except Exception as e:
            logger.error(f"Failed to initialize Redis clusters: {e}")
            raise
    
    async def _initialize_encryption_system(self):
        """Initialize military-grade encryption system"""
        try:
            # Generate master encryption keys
            await self._generate_master_keys()
            
            # Initialize encryption algorithms
            self.fernet_cipher = Fernet(self.encryption_keys["aes_master"].key_data)
            
            # Initialize Argon2 for password hashing
            self.argon2_hasher = argon2.PasswordHasher(
                time_cost=self.encryption_config["argon2_time_cost"],
                memory_cost=self.encryption_config["argon2_memory_cost"],
                parallelism=self.encryption_config["argon2_parallelism"]
            )
            
            logger.info("âœ… Encryption system initialized")
            
        except Exception as e:
            logger.error(f"Failed to initialize encryption system: {e}")
            raise
    
    async def _generate_master_keys(self):
        """Generate master encryption keys"""
        try:
            # AES-256 master key
            aes_key = Fernet.generate_key()
            self.encryption_keys["aes_master"] = EncryptionKey(
                key_id="aes_master",
                algorithm="AES-256",
                key_data=aes_key,
                created_at=datetime.utcnow(),
                expires_at=datetime.utcnow() + timedelta(days=self.encryption_config["key_rotation_days"]),
                is_active=True
            )
            
            # RSA-4096 key pair
            private_key = rsa.generate_private_key(
                public_exponent=65537,
                key_size=self.encryption_config["rsa_key_size"],
                backend=default_backend()
            )
            
            public_key = private_key.public_key()
            
            # Serialize keys
            private_pem = private_key.private_bytes(
                encoding=serialization.Encoding.PEM,
                format=serialization.PrivateFormat.PKCS8,
                encryption_algorithm=serialization.NoEncryption()
            )
            
            public_pem = public_key.public_bytes(
                encoding=serialization.Encoding.PEM,
                format=serialization.PublicFormat.SubjectPublicKeyInfo
            )
            
            self.encryption_keys["rsa_private"] = EncryptionKey(
                key_id="rsa_private",
                algorithm="RSA-4096",
                key_data=private_pem,
                created_at=datetime.utcnow(),
                expires_at=datetime.utcnow() + timedelta(days=self.encryption_config["key_rotation_days"]),
                is_active=True
            )
            
            self.encryption_keys["rsa_public"] = EncryptionKey(
                key_id="rsa_public",
                algorithm="RSA-4096",
                key_data=public_pem,
                created_at=datetime.utcnow(),
                expires_at=datetime.utcnow() + timedelta(days=self.encryption_config["key_rotation_days"]),
                is_active=True
            )
            
            # Store keys in Redis with encryption
            await self._store_encryption_keys()
            
        except Exception as e:
            logger.error(f"Failed to generate master keys: {e}")
            raise
    
    async def _store_encryption_keys(self):
        """Store encryption keys securely"""
        try:
            encryption_redis = self.redis_clusters["encryption"]
            
            for key_id, key in self.encryption_keys.items():
                key_data = {
                    "algorithm": key.algorithm,
                    "key_data": base64.b64encode(key.key_data).decode(),
                    "created_at": key.created_at.isoformat(),
                    "expires_at": key.expires_at.isoformat() if key.expires_at else None,
                    "is_active": key.is_active
                }
                
                await encryption_redis.setex(
                    f"encryption_key:{key_id}",
                    86400 * self.encryption_config["key_rotation_days"],
                    json.dumps(key_data)
                )
            
        except Exception as e:
            logger.error(f"Failed to store encryption keys: {e}")
            raise
    
    async def _initialize_connection_pools(self):
        """Initialize optimized connection pools"""
        try:
            # Connection pool monitoring
            for shard, engine in self.engines.items():
                pool = engine.pool
                self.connection_pools[shard] = {
                    "pool": pool,
                    "size": pool.size(),
                    "checked_in": pool.checkedin(),
                    "checked_out": pool.checkedout(),
                    "overflow": pool.overflow(),
                    "invalid": pool.invalid()
                }
            
            logger.info("âœ… Connection pools initialized")
            
        except Exception as e:
            logger.error(f"Failed to initialize connection pools: {e}")
            raise
    
    async def get_session(self, shard: DatabaseShard) -> AsyncSession:
        """Get database session for specific shard"""
        try:
            session_factory = self.sessions[shard]
            return session_factory()
        except Exception as e:
            logger.error(f"Failed to get session for shard {shard}: {e}")
            raise
    
    async def encrypt_data(self, data: str, level: EncryptionLevel) -> str:
        """Encrypt data with specified security level"""
        try:
            if level == EncryptionLevel.PUBLIC:
                return data
            
            elif level == EncryptionLevel.INTERNAL:
                # AES-256 encryption
                encrypted = self.fernet_cipher.encrypt(data.encode())
                return base64.b64encode(encrypted).decode()
            
            elif level == EncryptionLevel.SENSITIVE:
                # AES-256 + RSA-4096 encryption
                aes_encrypted = self.fernet_cipher.encrypt(data.encode())
                
                # Encrypt AES key with RSA
                rsa_private_key = serialization.load_pem_private_key(
                    self.encryption_keys["rsa_private"].key_data,
                    password=None,
                    backend=default_backend()
                )
                
                rsa_encrypted = rsa_private_key.encrypt(
                    self.encryption_keys["aes_master"].key_data,
                    padding.OAEP(
                        mgf=padding.MGF1(algorithm=hashes.SHA256()),
                        algorithm=hashes.SHA256(),
                        label=None
                    )
                )
                
                combined = {
                    "aes_data": base64.b64encode(aes_encrypted).decode(),
                    "rsa_key": base64.b64encode(rsa_encrypted).decode()
                }
                
                return base64.b64encode(json.dumps(combined).encode()).decode()
            
            elif level == EncryptionLevel.CRITICAL:
                # AES-256 + RSA-4096 + Argon2
                # First hash with Argon2
                argon2_hash = self.argon2_hasher.hash(data)
                
                # Then encrypt with AES + RSA
                return await self.encrypt_data(argon2_hash, EncryptionLevel.SENSITIVE)
            
            return data
            
        except Exception as e:
            logger.error(f"Failed to encrypt data: {e}")
            raise
    
    async def decrypt_data(self, encrypted_data: str, level: EncryptionLevel) -> str:
        """Decrypt data with specified security level"""
        try:
            if level == EncryptionLevel.PUBLIC:
                return encrypted_data
            
            elif level == EncryptionLevel.INTERNAL:
                # AES-256 decryption
                decoded = base64.b64decode(encrypted_data.encode())
                decrypted = self.fernet_cipher.decrypt(decoded)
                return decrypted.decode()
            
            elif level == EncryptionLevel.SENSITIVE:
                # AES-256 + RSA-4096 decryption
                combined_data = json.loads(base64.b64decode(encrypted_data.encode()).decode())
                
                # Decrypt RSA key
                rsa_private_key = serialization.load_pem_private_key(
                    self.encryption_keys["rsa_private"].key_data,
                    password=None,
                    backend=default_backend()
                )
                
                aes_key = rsa_private_key.decrypt(
                    base64.b64decode(combined_data["rsa_key"]),
                    padding.OAEP(
                        mgf=padding.MGF1(algorithm=hashes.SHA256()),
                        algorithm=hashes.SHA256(),
                        label=None
                    )
                )
                
                # Decrypt AES data
                fernet = Fernet(aes_key)
                decrypted = fernet.decrypt(base64.b64decode(combined_data["aes_data"]))
                return decrypted.decode()
            
            elif level == EncryptionLevel.CRITICAL:
                # Decrypt AES + RSA first
                decrypted = await self.decrypt_data(encrypted_data, EncryptionLevel.SENSITIVE)
                # Argon2 verification would be done here
                return decrypted
            
            return encrypted_data
            
        except Exception as e:
            logger.error(f"Failed to decrypt data: {e}")
            raise
    
    async def hash_password(self, password: str) -> str:
        """Hash password with Argon2"""
        try:
            return self.argon2_hasher.hash(password)
        except Exception as e:
            logger.error(f"Failed to hash password: {e}")
            raise
    
    async def verify_password(self, password: str, hashed_password: str) -> bool:
        """Verify password with Argon2"""
        try:
            self.argon2_hasher.verify(hashed_password, password)
            return True
        except argon2.exceptions.VerifyMismatchError:
            return False
        except Exception as e:
            logger.error(f"Failed to verify password: {e}")
            return False
    
    async def cache_set(self, key: str, value: Any, ttl: int = 3600, cluster: str = "primary"):
        """Set cache value with TTL"""
        try:
            redis_cluster = self.redis_clusters[cluster]
            
            if isinstance(value, (dict, list)):
                value = json.dumps(value, default=str)
            
            await redis_cluster.setex(key, ttl, value)
            
        except Exception as e:
            logger.error(f"Failed to set cache: {e}")
    
    async def cache_get(self, key: str, cluster: str = "primary") -> Optional[Any]:
        """Get cache value"""
        try:
            redis_cluster = self.redis_clusters[cluster]
            value = await redis_cluster.get(key)
            
            if value is None:
                return None
            
            # Try to parse as JSON
            try:
                return json.loads(value)
            except (json.JSONDecodeError, TypeError):
                return value
                
        except Exception as e:
            logger.error(f"Failed to get cache: {e}")
            return None
    
    async def cache_delete(self, key: str, cluster: str = "primary"):
        """Delete cache value"""
        try:
            redis_cluster = self.redis_clusters[cluster]
            await redis_cluster.delete(key)
        except Exception as e:
            logger.error(f"Failed to delete cache: {e}")
    
    async def execute_query(self, shard: DatabaseShard, query: str, params: Dict[str, Any] = None) -> List[Dict[str, Any]]:
        """Execute optimized query with monitoring"""
        try:
            start_time = asyncio.get_event_loop().time()
            
            async with self.get_session(shard) as session:
                result = await session.execute(text(query), params or {})
                rows = result.fetchall()
                
                # Convert to dict
                columns = result.keys()
                data = [dict(zip(columns, row)) for row in rows]
                
                # Record metrics
                execution_time = asyncio.get_event_loop().time() - start_time
                await self._record_query_metrics(shard, query, execution_time, len(data))
                
                return data
                
        except Exception as e:
            logger.error(f"Failed to execute query: {e}")
            raise
    
    async def _record_query_metrics(self, shard: DatabaseShard, query: str, execution_time: float, row_count: int):
        """Record query performance metrics"""
        try:
            query_hash = hashlib.md5(query.encode()).hexdigest()
            
            if shard not in self.query_metrics:
                self.query_metrics[shard] = {}
            
            if query_hash not in self.query_metrics[shard]:
                self.query_metrics[shard][query_hash] = {
                    "query": query,
                    "execution_times": [],
                    "row_counts": [],
                    "total_executions": 0,
                    "avg_execution_time": 0.0,
                    "avg_row_count": 0.0
                }
            
            metrics = self.query_metrics[shard][query_hash]
            metrics["execution_times"].append(execution_time)
            metrics["row_counts"].append(row_count)
            metrics["total_executions"] += 1
            
            # Keep only last 100 executions
            if len(metrics["execution_times"]) > 100:
                metrics["execution_times"] = metrics["execution_times"][-100:]
                metrics["row_counts"] = metrics["row_counts"][-100:]
            
            # Update averages
            metrics["avg_execution_time"] = sum(metrics["execution_times"]) / len(metrics["execution_times"])
            metrics["avg_row_count"] = sum(metrics["row_counts"]) / len(metrics["row_counts"])
            
        except Exception as e:
            logger.error(f"Failed to record query metrics: {e}")
    
    async def _health_monitor(self):
        """Monitor database health"""
        while True:
            try:
                await asyncio.sleep(30)  # Check every 30 seconds
                
                # Check database connections
                for shard, engine in self.engines.items():
                    try:
                        async with engine.begin() as conn:
                            await conn.execute(text("SELECT 1"))
                        
                        # Update connection metrics
                        pool = engine.pool
                        self.connection_metrics[shard] = {
                            "status": "healthy",
                            "pool_size": pool.size(),
                            "checked_in": pool.checkedin(),
                            "checked_out": pool.checkedout(),
                            "overflow": pool.overflow(),
                            "invalid": pool.invalid(),
                            "last_check": datetime.utcnow()
                        }
                        
                    except Exception as e:
                        logger.error(f"Database health check failed for {shard}: {e}")
                        self.connection_metrics[shard] = {
                            "status": "unhealthy",
                            "error": str(e),
                            "last_check": datetime.utcnow()
                        }
                
                # Check Redis clusters
                for name, cluster in self.redis_clusters.items():
                    try:
                        await cluster.ping()
                        logger.debug(f"Redis cluster {name} is healthy")
                    except Exception as e:
                        logger.error(f"Redis cluster {name} health check failed: {e}")
                
            except Exception as e:
                logger.error(f"Health monitor error: {e}")
                await asyncio.sleep(30)
    
    async def _performance_monitor(self):
        """Monitor database performance"""
        while True:
            try:
                await asyncio.sleep(60)  # Monitor every minute
                
                # Analyze slow queries
                for shard, metrics in self.query_metrics.items():
                    for query_hash, query_metrics in metrics.items():
                        if query_metrics["avg_execution_time"] > 1.0:  # > 1 second
                            logger.warning(f"Slow query detected in {shard}: {query_metrics['avg_execution_time']:.2f}s")
                
                # Check connection pool utilization
                for shard, metrics in self.connection_metrics.items():
                    if metrics["status"] == "healthy":
                        utilization = (metrics["checked_out"] / metrics["pool_size"]) * 100
                        if utilization > 80:
                            logger.warning(f"High connection pool utilization in {shard}: {utilization:.1f}%")
                
            except Exception as e:
                logger.error(f"Performance monitor error: {e}")
                await asyncio.sleep(60)
    
    async def _encryption_key_rotator(self):
        """Rotate encryption keys periodically"""
        while True:
            try:
                await asyncio.sleep(86400)  # Check daily
                
                current_time = datetime.utcnow()
                
                for key_id, key in self.encryption_keys.items():
                    if key.expires_at and current_time >= key.expires_at:
                        logger.info(f"Rotating encryption key: {key_id}")
                        await self._rotate_encryption_key(key_id)
                
            except Exception as e:
                logger.error(f"Encryption key rotator error: {e}")
                await asyncio.sleep(86400)
    
    async def _rotate_encryption_key(self, key_id: str):
        """Rotate specific encryption key"""
        try:
            # Generate new key
            if key_id == "aes_master":
                new_key = Fernet.generate_key()
                algorithm = "AES-256"
            elif key_id in ["rsa_private", "rsa_public"]:
                # Generate new RSA key pair
                private_key = rsa.generate_private_key(
                    public_exponent=65537,
                    key_size=self.encryption_config["rsa_key_size"],
                    backend=default_backend()
                )
                
                if key_id == "rsa_private":
                    new_key = private_key.private_bytes(
                        encoding=serialization.Encoding.PEM,
                        format=serialization.PrivateFormat.PKCS8,
                        encryption_algorithm=serialization.NoEncryption()
                    )
                else:
                    new_key = private_key.public_key().public_bytes(
                        encoding=serialization.Encoding.PEM,
                        format=serialization.PublicFormat.SubjectPublicKeyInfo
                    )
                algorithm = "RSA-4096"
            else:
                return
            
            # Update key
            self.encryption_keys[key_id] = EncryptionKey(
                key_id=key_id,
                algorithm=algorithm,
                key_data=new_key,
                created_at=datetime.utcnow(),
                expires_at=datetime.utcnow() + timedelta(days=self.encryption_config["key_rotation_days"]),
                is_active=True
            )
            
            # Store in Redis
            await self._store_encryption_keys()
            
            logger.info(f"Encryption key rotated successfully: {key_id}")
            
        except Exception as e:
            logger.error(f"Failed to rotate encryption key {key_id}: {e}")
    
    async def _backup_scheduler(self):
        """Schedule database backups"""
        while True:
            try:
                await asyncio.sleep(3600)  # Check every hour
                
                # Schedule backups based on time
                current_hour = datetime.utcnow().hour
                
                if current_hour == 2:  # 2 AM - Full backup
                    await self._perform_full_backup()
                elif current_hour % 6 == 0:  # Every 6 hours - Incremental backup
                    await self._perform_incremental_backup()
                
            except Exception as e:
                logger.error(f"Backup scheduler error: {e}")
                await asyncio.sleep(3600)
    
    async def _perform_full_backup(self):
        """Perform full database backup"""
        try:
            logger.info("Starting full database backup...")
            
            for shard in DatabaseShard:
                # This would implement actual backup logic
                # For now, just log
                logger.info(f"Full backup completed for shard: {shard.value}")
            
            logger.info("Full database backup completed")
            
        except Exception as e:
            logger.error(f"Full backup failed: {e}")
    
    async def _perform_incremental_backup(self):
        """Perform incremental database backup"""
        try:
            logger.info("Starting incremental database backup...")
            
            for shard in DatabaseShard:
                # This would implement actual incremental backup logic
                logger.info(f"Incremental backup completed for shard: {shard.value}")
            
            logger.info("Incremental database backup completed")
            
        except Exception as e:
            logger.error(f"Incremental backup failed: {e}")
    
    async def get_performance_metrics(self) -> Dict[str, Any]:
        """Get comprehensive performance metrics"""
        try:
            return {
                "database_metrics": {
                    "shards": {
                        shard.value: {
                            "status": self.connection_metrics.get(shard, {}).get("status", "unknown"),
                            "pool_utilization": self._calculate_pool_utilization(shard),
                            "query_metrics": self.query_metrics.get(shard, {})
                        }
                        for shard in DatabaseShard
                    }
                },
                "redis_metrics": {
                    "clusters": {
                        name: {
                            "status": "healthy" if await cluster.ping() else "unhealthy"
                        }
                        for name, cluster in self.redis_clusters.items()
                    }
                },
                "encryption_metrics": {
                    "active_keys": len([k for k in self.encryption_keys.values() if k.is_active]),
                    "key_rotation_status": "healthy"
                }
            }
            
        except Exception as e:
            logger.error(f"Failed to get performance metrics: {e}")
            return {}
    
    def _calculate_pool_utilization(self, shard: DatabaseShard) -> float:
        """Calculate connection pool utilization"""
        try:
            metrics = self.connection_metrics.get(shard, {})
            if metrics.get("status") != "healthy":
                return 0.0
            
            pool_size = metrics.get("pool_size", 1)
            checked_out = metrics.get("checked_out", 0)
            
            return (checked_out / pool_size) * 100 if pool_size > 0 else 0.0
            
        except Exception as e:
            logger.error(f"Failed to calculate pool utilization: {e}")
            return 0.0


# Global enterprise database instance
enterprise_database = EnterpriseDatabase()
